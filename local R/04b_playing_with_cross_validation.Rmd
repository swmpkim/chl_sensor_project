---
title: "Convincing myself I know how k-fold validation works"
author: "Kim Cressman"
date: "12/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(boot)
```

# simulate some data  

```{r}
x1 = rnorm(1000)
x2 = rnorm(1000, mean = 5, sd = 4)
x3 = rnorm(1000, mean = 20, sd = 8)
noise <- rnorm(1000, mean = 5, sd = 1)

y = 8 + 2*x1 + 0.5*x2 + 0.2*x3 + noise

dat <- data.frame(cbind(y, x1, x2, x3, noise))
```

using `boot::cv.glm()`:

```{r}
reglm <- lm(y ~ x1 + x2 + x3, data = dat)

cvglm <- glm(y ~ x1 + x2 + x3, data = dat)

kfoldcv <- cv.glm(dat, cvglm, K = 5)
kfoldcv$delta


cost <- function(obs, exp) median(abs(obs - exp))
kfold2 <- cv.glm(dat, cvglm, cost, K=5)
kfold2$delta
```


generate my own version of predictive error with a loop:  

```{r}
# split data into 5 sets randomly
groups <- rep(1:5, length.out = nrow(dat))
dat$group <- sample(groups, replace = FALSE)
MSEs <- rep(0, 5)
MADs <- rep(0, 5)

for(i in 1:5){
    subdat <- filter(dat, group != i)
    subtest <- filter(dat, group == i)
    
    sublm <- lm(y ~ x1 + x2 + x3, data = subdat)
    subpreds <- predict(sublm, newdata = subtest)
    MSEs[i] <- mean((subpreds-subtest$y)^2)
    MADs[i] <- median(abs(subpreds - subtest$y))
}

MSEs
mean(MSEs)
MADs
mean(MADs)
```

```{r}
# could write my own cost function: let's say median absolute deviance
# of "actual" predictions (squared model output)

dat$y2 <- y^2

lmsq <- lm(sqrt(y2) ~ x1 + x2 + x3, data = dat)

glmsq <- glm(sqrt(y2) ~ x1 + x2 + x3, data = dat)

kfoldsq <- cv.glm(dat, glmsq, K=5)
kfoldsq$delta

```

```{r}
groups <- rep(1:5, length.out = ncol(dat))
dat$group <- sample(groups, replace = FALSE)
MSEs <- rep(0, 5)
MADs <- rep(0, 5)

for(i in 1:5){
    subdat <- filter(dat, group != i)
    subtest <- filter(dat, group == i)
    
    sublm <- lm(sqrt(y2) ~ x1 + x2 + x3, data = subdat)
    subpreds <- predict(sublm, newdata = subtest)
    MSEs[i] <- mean((subpreds-sqrt(subtest$y2))^2)
    MADs[i] <- median(abs(subpreds^2 - subtest$y2))
}

MSEs
mean(MSEs)
MADs
mean(MADs)
cost <- function(obs, expt) median(abs(obs^2 - expt^2))
kfoldsq2 <- cv.glm(dat, glmsq, cost, K=5)
kfoldsq2$delta
```


# Symmetric Median Absolute Percentage Error  

See Hyndman and Koehler 2006. Another look at measures of forecast accuracy. International Journal of Forecasting 22(4): 679-688.  

Which is actually more of a cautionary tale about most of these measures, but it explains them all so well (especially in combination with some other papers) that I think sMdAPE is appropriate for my needs.  

Formula from Makridakis 1993. Accuracy measures: theoretical and practical concerns. International Journal of Forecasting 9: 527-529.  

That formula seems a little different than what Hyndman and Koehler presented; particularly in that it can't lead to negative values. And rather than taking the mean, I'm just taking the median.   

## code the cost function  

```{r}
sMdAPE <- function(obs, expt){
    median(
        abs(
            100 * (obs-expt) / ((obs+expt)/2)
        )
    )
}
```

```{r}
sMdAPE(obs = subtest$y, expt = subpreds)

dat3 <- data.frame("obs" = subtest$y,
                   "expt" = subpreds)

dat4 <- dat3 %>% 
    mutate(diff = obs-expt,
           pctdiff = 100*diff/((obs+expt)/2),
           APE = abs(pctdiff))

```

## see if i can make it work on squared stuff  

Because the predictions are the square root of what we're actually interested in, and i want *prediction* accuracy.  

```{r}
cost <- function(obs, expt){
    median(
        abs(
            100 * (obs^2-expt^2) / ((obs^2+expt^2)/2)
        )
    )
}
```

Test on my toy data:  

```{r}
dat5 <- subtest %>% 
    mutate(preds = subpreds^2,
           diff = y2-preds,
           pctdiff = 100*diff/((y2+preds)/2),
           APE = abs(pctdiff))
median(dat5$APE)

# test on the unsquared ones
cost(subtest$y, subpreds)
```

