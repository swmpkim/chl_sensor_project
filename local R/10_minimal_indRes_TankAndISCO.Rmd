---
output: 
    html_document
---

```{r}
load(here::here("data", "subset_no_NA.RData"))
table(dat$method)
# dat <- filter(dat, method == "tank")

# original plot settings, for later
op <- par()

# options, for MuMIn operations
# will prevent fitting sub-models to different datasets (from help file)
options(na.action = "na.fail")
```

```{r}
dat$response <- sqrt(dat$extracted)
```


## Modeling: Individual Reserves, Tank + ISCO  

Each of the models described below are fitted and evaluated using symmetric Median Absolute Percent Error.  

**Full Model**  

Includes all interactions between sensor_rfu, turbidity, and FDOM. Additionally, temp\*rfu and temp\*fdom. Hoping we can actually drop some of these.  

Reserve included as fixed effect.  

response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu\*turb\*fdom_qsu + sensor_rfu\*turb + sensor_rfu\*fdom_qsu + turb\*fdom_qsu + temp + sensor_rfu\*temp + fdom_qsu\*temp + season + reserve


**No FDOM**  

If we drop FDOM and all its interactions, what do we get?  

**NOTE** this still includes both reserve and season (as well as sensor readings from temp and turbidity) 


**RFU only**  

This does *not* include reserve or season. If the other models above are better, reserve or season could still be the reason (vs. turbidity/temp/FDOM corrections). It would mean **local factors still have to be accounted for and modelled to get the most out of a TAL sensor**.  



Using leave-one-out cross-validation due to low number of samples for some reserves and potential to introduce new factor levels on validation set.  

```{r}
mod_sel_out <- list()
mod_summs_out <- list()
reserves <- unique(dat$reserve)
set.seed(1216)

for(i in seq_along(reserves)){
    res <- dat %>% 
        filter(reserve == reserves[i])

    if(length(unique(res$season))>=2){  
        
        full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp + season, data = res)
        no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp + season, data = res)
        no_fdom_lm <- lm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp + season, data = res)
                           
                           } else {
                               
            full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp, data = res)
            
            no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp, data = res)
            no_fdom_lm <- lm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp, data = res)
        }
    
    rfu_only <- glm(response ~ sensor_rfu, data = res)
    rfu_only_lm <- lm(response ~ sensor_rfu, data = res)
    
    sel <- dredge(full_ols, 
                  fixed = "sensor_rfu",
                  extra = c("R^2", "AdjR^2" = function(x) summary(x)$adj.r.squared))
    
    # perform cross-validation on top model to find median prediction error
    ols.1 <- get.models(sel, 1)[[1]]
    ols.cv <- cv.glm(res, ols.1, cost)
    
    no_fdom.cv <- cv.glm(res, no_fdom_ols, cost)
    no_fdom.df <- data.frame(model.sel(no_fdom_ols))
    no_fdom.df$pred_test_error <- round(no_fdom.cv$delta[1], 2)
    
    rfu_only.cv <- cv.glm(res, rfu_only, cost)
    rfu_only.df <- data.frame(model.sel(rfu_only))
    rfu_only.df$pred_test_error <- round(rfu_only.cv$delta[1], 2)
    
    # pull out R^2s from lms (glms were run for cross-validation but really they're just regular linear models)
    no_fdom.df$R.2 <- round(summary(no_fdom_lm)$r.squared, 4)
    rfu_only.df$R.2 <- round(summary(rfu_only_lm)$r.squared, 4)
    

    # see anything with delta AICc < 2
    # subdf <- data.frame(subset(sel, delta < 2))
    
    # only top one
    subdf <- data.frame(sel[1])
    subdf$pred_test_error <- round(ols.cv$delta[1], 2)
    subdf <- subdf %>% 
      bind_rows(no_fdom.df, rfu_only.df) %>% 
      rownames_to_column(var = "model") %>% 
      mutate(model = case_when(model == "...1" ~ "best_AIC",
                               TRUE ~ model)) %>% 
      select(model, AICc, R.2, pred_test_error, everything()) %>% 
      select(-c(delta, weight, family))
    
    # attach prediction error to it
    mod_sel_out[[i]] <- subdf
    
    # summary for top model
    mod_summs_out[[i]] <- summary(ols.1)
    
      
}
```

```{r}
mod_sel_out <- purrr::set_names(mod_sel_out, reserves)
mod_summs_out <- purrr::set_names(mod_summs_out, reserves)
```


## Summary Table  


```{r}
mod_sel_df <- bind_rows(mod_sel_out, .id = "Reserve") %>% 
  select(Reserve, model, AICc, R.2, prediction_error = pred_test_error,
         sensor_rfu, fdom_qsu, turb, 
         season, temp, everything()) %>% 
  mutate(across(c(AICc, prediction_error), round, 2),
           across(c(R.2, sensor_rfu, X.Intercept.), round, 3),
           across(c(fdom_qsu, turb, temp), round, 4),
           across(c(fdom_qsu.sensor_rfu:fdom_qsu.sensor_rfu.turb), round, 4) )


knitr::kable(mod_sel_df)

write.csv(mod_sel_df, file = here::here("model_coeffs",                                     "IndRes_TankAndISCO.csv"), row.names = FALSE)
```

