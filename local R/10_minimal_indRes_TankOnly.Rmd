---
output: 
    html_document
---

```{r}
load(here::here("data", "subset_no_NA.RData"))
table(dat$method)
dat_isco <- filter(dat, method == "isco")
dat <- filter(dat, method == "tank")

# subset isco data to only reserves that also have tank data
dat_isco <- dat_isco %>% 
  filter(reserve %in% unique(dat$reserve))

# original plot settings, for later
op <- par()

# options, for MuMIn operations
# will prevent fitting sub-models to different datasets (from help file)
options(na.action = "na.fail")
```

```{r}
dat$response <- sqrt(dat$extracted)
```


## Modeling: Individual Reserves, Tank Only  

Each of the models described below are fitted and evaluated using symmetric Median Absolute Percent Error.  

**Full Model**  

Includes all interactions between sensor_rfu, turbidity, and FDOM. Additionally, temp\*rfu and temp\*fdom. Hoping we can actually drop some of these.  

Reserve included as fixed effect.  

response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu\*turb\*fdom_qsu + sensor_rfu\*turb + sensor_rfu\*fdom_qsu + turb\*fdom_qsu + temp + sensor_rfu\*temp + fdom_qsu\*temp + season + reserve


**No FDOM**  

If we drop FDOM and all its interactions, what do we get?  

**NOTE** this still includes both reserve and season (as well as sensor readings from temp and turbidity) 


**RFU only**  

This does *not* include reserve or season. If the other models above are better, reserve or season could still be the reason (vs. turbidity/temp/FDOM corrections). It would mean **local factors still have to be accounted for and modelled to get the most out of a TAL sensor**.  



Using leave-one-out cross-validation due to low number of samples for some reserves and potential to introduce new factor levels on validation set.  


**Testing predictions on ISCO data**  

Five reserves collected both tank and ISCO data: GND, GTM, LKS, PDB, GRB. (NIW is not included here because their ISCO data was collected at a different station than their tank data.)  

For these reserves, we will see how well predictions from the three core models built from tank data compare to the observed ISCO data.  

```{r}
mod_sel_out <- list()
mod_summs_out <- list()
isco_preds_out <- list()
reserves <- unique(dat$reserve)
set.seed(1216)

for(i in seq_along(reserves)){
    res <- dat %>% 
        filter(reserve == reserves[i])

    if(length(unique(res$season))>=2){  
        
        full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp + season, data = res)
        no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp + season, data = res)
        no_fdom_lm <- lm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp + season, data = res)
                           
                           } else {
                               
            full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp, data = res)
            
            no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp, data = res)
            no_fdom_lm <- lm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp, data = res)
        }
    
    rfu_only <- glm(response ~ sensor_rfu, data = res)
    rfu_only_lm <- lm(response ~ sensor_rfu, data = res)
    
    sel <- dredge(full_ols, 
                  fixed = "sensor_rfu",
                  extra = c("R^2", "AdjR^2" = function(x) summary(x)$adj.r.squared))
    
    # perform cross-validation on top model to find median prediction error
    ols.1 <- get.models(sel, 1)[[1]]
    ols.cv <- cv.glm(res, ols.1, cost)
    
    no_fdom.cv <- cv.glm(res, no_fdom_ols, cost)
    no_fdom.df <- data.frame(model.sel(no_fdom_ols))
    no_fdom.df$pred_test_error <- round(no_fdom.cv$delta[1], 2)
    
    rfu_only.cv <- cv.glm(res, rfu_only, cost)
    rfu_only.df <- data.frame(model.sel(rfu_only))
    rfu_only.df$pred_test_error <- round(rfu_only.cv$delta[1], 2)
    
    # pull out R^2s from lms (glms were run for cross-validation but really they're just regular linear models)
    no_fdom.df$R.2 <- round(summary(no_fdom_lm)$r.squared, 4)
    rfu_only.df$R.2 <- round(summary(rfu_only_lm)$r.squared, 4)
    

    # see anything with delta AICc < 2
    # subdf <- data.frame(subset(sel, delta < 2))
    
    # only top one
    subdf <- data.frame(sel[1])
    subdf$pred_test_error <- round(ols.cv$delta[1], 2)
    subdf <- subdf %>% 
      bind_rows(no_fdom.df, rfu_only.df) %>% 
      rownames_to_column(var = "model") %>% 
      mutate(model = case_when(model == "...1" ~ "best_AIC",
                               TRUE ~ model)) %>% 
      select(model, AICc, R.2, pred_test_error, everything()) %>% 
      select(-c(delta, weight, family))
    
    # attach prediction error to it
    mod_sel_out[[i]] <- subdf
    
    # summary for top model
    mod_summs_out[[i]] <- summary(ols.1)
    
    
    # ISCO predictions 
    if(reserves[i] %in% unique(dat_isco$reserve)){
      isco <- dat_isco %>% 
        filter(reserve == reserves[i],
               season %in% unique(res$season)) %>% 
        mutate(preds_best_AIC = (predict(ols.1, newdata = .))^2,
               preds_no_fdom_ols = (predict(no_fdom_ols, newdata = .))^2,
               preds_rfu_only = (predict(rfu_only, newdata = .))^2)
      
      isco_preds_out[[i]] <- isco
      
      
    }
    
      
}
```

```{r}
mod_sel_out <- purrr::set_names(mod_sel_out, reserves)
mod_summs_out <- purrr::set_names(mod_summs_out, reserves)
isco_preds <- bind_rows(isco_preds_out)
isco_preds_long <- isco_preds %>% 
  select(reserve, month, season, method, extracted, starts_with("preds")) %>% 
  pivot_longer(cols = starts_with("preds"),
               names_to = "model",
               names_prefix = "preds_",
               values_to = "predicted")
```

## Summary Table  


```{r}
mod_sel_df <- bind_rows(mod_sel_out, .id = "Reserve") %>% 
  select(Reserve, model, AICc, R.2, prediction_error = pred_test_error,
         sensor_rfu, fdom_qsu, turb, 
         season, temp, everything()) %>% 
  mutate(across(c(AICc, prediction_error), round, 2),
           across(c(R.2, sensor_rfu, X.Intercept.), round, 3),
           across(c(fdom_qsu, turb, temp), round, 4),
           across(c(fdom_qsu.sensor_rfu:fdom_qsu.sensor_rfu.turb), round, 4) )


knitr::kable(mod_sel_df)

write.csv(mod_sel_df, file = here::here("model_coeffs",                                     "IndRes_TankOnly.csv"), row.names = FALSE)
```

## Graphs of ISCO predictions  

```{r}
ggplot(isco_preds_long) +
  geom_abline(slope = 1, intercept = 0, color = "gray40") +
  geom_point(aes(x = predicted, y = extracted, color = reserve, shape = model),
             alpha = 0.6, size = 2) +
  facet_wrap(~model) +
  labs(title = "Observed ISCO by predictions from tank models",
       subtitle = "Gray line on each panel is 1:1 line")
```

```{r}
ggplot(isco_preds_long) +
  geom_abline(slope = 1, intercept = 0, color = "gray40") +
  geom_point(aes(x = predicted, y = extracted, color = model, shape = model),
             alpha = 0.6, size = 2) +
  facet_wrap(~reserve) +
  labs(title = "Observed ISCO by predictions from tank models",
       subtitle = "Gray line on each panel is 1:1 line")
```

```{r}
ggplot(isco_preds_long) +
  geom_abline(slope = 1, intercept = 0, color = "gray40") +
  geom_point(aes(x = predicted, y = extracted, color = model, shape = model),
             alpha = 0.6, size = 2) +
  facet_wrap(~reserve, scales = "free") +
  labs(title = "Observed ISCO by predictions from tank models",
       subtitle = "Free axes; Gray line on each panel is 1:1 line")
```

