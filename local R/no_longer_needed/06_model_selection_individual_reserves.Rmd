---
title: "Individual Reserve Models"
date: "original: 2021-12-09; latest update: `r Sys.Date()`"
output: 
    html_document:
        code_folding: hide
        toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r}
library(readxl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(lmerTest)
library(MuMIn)
library(boot)

load(here::here("data", "subset_no_NA.RData"))

# original plot settings, for later
op <- par()

# options, for MuMIn operations
# will prevent fitting sub-models to different datasets (from help file)
options(na.action = "na.fail")
```

```{r}
dat$response <- sqrt(dat$extracted)
```


For each reserve, want to pull out that data, model, and dredge.  

```{r}
# hand-written cost function for cross-validation
# customized because the response variable was a square root
# so in this function, am squaring both observed and expected values
# to get the back-transformed predictions
cost <- function(obs, expt){
    median(
        abs(
            100 * (obs^2-expt^2) / ((obs^2+expt^2)/2)
        )
    )
}
```


# Loop through reserves and run models  

```{r}
mod_sel_out <- list()
mod_summs_out <- list()
reserves <- unique(dat$reserve)
set.seed(1216)

for(i in seq_along(reserves)){
    res <- dat %>% 
        filter(reserve == reserves[i])

    if(length(unique(res$season))>=2){  
        
        full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp + season, data = res)
        no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp + season, data = res)
                           
                           } else {
                               
            full_ols <- glm(response ~ sensor_rfu + turb + fdom_qsu + sensor_rfu*turb*fdom_qsu + sensor_rfu*turb + sensor_rfu*fdom_qsu + turb*fdom_qsu + temp + sensor_rfu*temp + fdom_qsu*temp, data = res)
            
            no_fdom_ols <- glm(response ~ sensor_rfu + turb +  sensor_rfu*turb + temp + sensor_rfu*temp, data = res)
        }
    
    rfu_only <- glm(response ~ sensor_rfu, data = res)
    
    sel <- dredge(full_ols, 
                  fixed = "sensor_rfu",
                  extra = c("R^2", "AdjR^2" = function(x) summary(x)$adj.r.squared))
    
    # perform cross-validation on top model to find median prediction error
    ols.1 <- get.models(sel, 1)[[1]]
    ols.cv <- cv.glm(res, ols.1, cost, K = 5)
    
    no_fdom.cv <- cv.glm(res, no_fdom_ols, cost, K=5)
    no_fdom.df <- data.frame(model.sel(no_fdom_ols))
    no_fdom.df$pred_test_error <- round(no_fdom.cv$delta[1], 2)
    
    rfu_only.cv <- cv.glm(res, rfu_only, cost, K=5)
    rfu_only.df <- data.frame(model.sel(rfu_only))
    rfu_only.df$pred_test_error <- round(rfu_only.cv$delta[1], 2)
    

    # see anything with delta AICc < 3
    subdf <- data.frame(subset(sel, delta < 3))
    subdf$pred_test_error <- NA_real_
    subdf$pred_test_error[1] <- round(ols.cv$delta[1], 2)
    subdf <- subdf %>% 
        bind_rows(no_fdom.df, rfu_only.df) %>% 
        select(AICc, R.2, pred_test_error, everything())
    
    # attach prediction error to it
    mod_sel_out[[i]] <- subdf
    
    # summary for top model
    mod_summs_out[[i]] <- summary(ols.1)
    
      
}
```

```{r}
mod_sel_out <- purrr::set_names(mod_sel_out, reserves)
mod_summs_out <- purrr::set_names(mod_summs_out, reserves)
```


# Model selection matrices by reserve  

```{r}
mod_sel_out
```


# Top model summary by reserve  

```{r}
mod_summs_out
```

